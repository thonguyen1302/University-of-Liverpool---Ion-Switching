{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 3)\n",
      "(2000000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "      <th>open_channels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>-2.7600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-2.8557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0003</td>\n",
       "      <td>-2.4074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0004</td>\n",
       "      <td>-3.1404</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>-3.1525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  signal  open_channels\n",
       "0  0.0001 -2.7600              0\n",
       "1  0.0002 -2.8557              0\n",
       "2  0.0003 -2.4074              0\n",
       "3  0.0004 -3.1404              0\n",
       "4  0.0005 -3.1525              0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0001</td>\n",
       "      <td>-2.6498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0002</td>\n",
       "      <td>-2.8494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.0003</td>\n",
       "      <td>-2.8600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.0004</td>\n",
       "      <td>-2.4350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0005</td>\n",
       "      <td>-2.6155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  signal\n",
       "0  500.0001 -2.6498\n",
       "1  500.0002 -2.8494\n",
       "2  500.0003 -2.8600\n",
       "3  500.0004 -2.4350\n",
       "4  500.0005 -2.6155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2058a77da58>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['open_channels'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN\n",
    "import numpy as np\n",
    "WINDOW_SIZE = 10\n",
    "def generator_data(train, batch_size, WINDOW_SIZE = 100):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for i in range(WINDOW_SIZE,train.shape[0]):\n",
    "        #print(\"DEBUG\", i)\n",
    "        feature = [[i]for i in train.to_numpy()[i-WINDOW_SIZE:i, 1]]\n",
    "        X_train.append(feature)\n",
    "        Y_train.append(int(train.to_numpy()[i,2]))\n",
    "        if (i-WINDOW_SIZE+1) % batch_size == 0:\n",
    "           # print(len(X_train))\n",
    "            #print(len(Y_train))\n",
    "            data_generate = (np.array(X_train), np.array(Y_train))\n",
    "            X_train = [] \n",
    "            Y_train = []\n",
    "            yield data_generate\n",
    "#data_generator = generator_data(train, batch_size=8, WINDOW_SIZE=WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-a46cfb8dc8dc>:10: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-8-a46cfb8dc8dc>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"GatherV2:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(?, 10), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.342873 Acc : 0.0\n",
      "Loss : 2.3086417 Acc : 0.0\n",
      "Loss : 2.275697 Acc : 0.0\n",
      "Loss : 2.2470624 Acc : 0.0\n",
      "Loss : 2.213388 Acc : 0.0\n",
      "Loss : 2.179083 Acc : 0.421875\n",
      "Loss : 2.146388 Acc : 0.953125\n",
      "Loss : 2.1123462 Acc : 1.0\n",
      "Loss : 2.0782735 Acc : 1.0\n",
      "Loss : 2.0426843 Acc : 1.0\n",
      "Loss : 2.0081012 Acc : 1.0\n",
      "Loss : 1.9725157 Acc : 1.0\n",
      "Loss : 1.9356244 Acc : 1.0\n",
      "Loss : 1.8974671 Acc : 1.0\n",
      "Loss : 1.8641326 Acc : 1.0\n",
      "Loss : 1.8230699 Acc : 1.0\n",
      "Loss : 1.7822576 Acc : 1.0\n",
      "Loss : 1.7453599 Acc : 1.0\n",
      "Loss : 1.7066011 Acc : 1.0\n",
      "Loss : 1.6600406 Acc : 1.0\n",
      "Loss : 1.6254511 Acc : 1.0\n",
      "Loss : 1.5846789 Acc : 1.0\n",
      "Loss : 1.5336003 Acc : 1.0\n",
      "Loss : 1.4946322 Acc : 1.0\n",
      "Loss : 1.4579569 Acc : 1.0\n",
      "Loss : 1.4068105 Acc : 1.0\n",
      "Loss : 1.361052 Acc : 1.0\n",
      "Loss : 1.3220298 Acc : 1.0\n",
      "Loss : 1.2765472 Acc : 1.0\n",
      "Loss : 1.2301517 Acc : 1.0\n",
      "Loss : 1.1889634 Acc : 1.0\n",
      "Loss : 1.1386237 Acc : 1.0\n",
      "Loss : 1.0947938 Acc : 1.0\n",
      "Loss : 1.0506144 Acc : 1.0\n",
      "Loss : 1.0045582 Acc : 1.0\n",
      "Loss : 0.9619026 Acc : 1.0\n",
      "Loss : 0.9291162 Acc : 1.0\n",
      "Loss : 0.88082784 Acc : 1.0\n",
      "Loss : 0.8319206 Acc : 1.0\n",
      "Loss : 0.79971075 Acc : 1.0\n",
      "Loss : 0.75520515 Acc : 1.0\n",
      "Loss : 0.71039784 Acc : 1.0\n",
      "Loss : 0.6843454 Acc : 1.0\n",
      "Loss : 0.64411235 Acc : 1.0\n",
      "Loss : 0.6049403 Acc : 1.0\n",
      "Loss : 0.5740217 Acc : 1.0\n",
      "Loss : 0.54276204 Acc : 1.0\n",
      "Loss : 0.51232666 Acc : 1.0\n",
      "Loss : 0.4823157 Acc : 1.0\n",
      "Loss : 0.45933998 Acc : 1.0\n",
      "Loss : 0.42856306 Acc : 1.0\n",
      "Loss : 0.40578264 Acc : 1.0\n",
      "Loss : 0.38257208 Acc : 1.0\n",
      "Loss : 0.35744268 Acc : 1.0\n",
      "Loss : 0.33856913 Acc : 1.0\n",
      "Loss : 0.31972238 Acc : 1.0\n",
      "Loss : 0.29968685 Acc : 1.0\n",
      "Loss : 0.28057343 Acc : 1.0\n",
      "Loss : 0.2679907 Acc : 1.0\n",
      "Loss : 0.25048274 Acc : 1.0\n",
      "Loss : 0.2349683 Acc : 1.0\n",
      "Loss : 0.22799282 Acc : 1.0\n",
      "Loss : 0.2114434 Acc : 1.0\n",
      "Loss : 0.19781879 Acc : 1.0\n",
      "Loss : 0.19027643 Acc : 1.0\n",
      "Loss : 0.17793827 Acc : 1.0\n",
      "Loss : 0.16621295 Acc : 1.0\n",
      "Loss : 0.15893334 Acc : 1.0\n",
      "Loss : 0.15096548 Acc : 1.0\n",
      "Loss : 0.14238752 Acc : 1.0\n",
      "Loss : 0.13456443 Acc : 1.0\n",
      "Loss : 0.12892158 Acc : 1.0\n",
      "Loss : 0.11978668 Acc : 1.0\n",
      "Loss : 0.1165369 Acc : 1.0\n",
      "Loss : 0.10996411 Acc : 1.0\n",
      "Loss : 0.10195081 Acc : 1.0\n",
      "Loss : 0.09796382 Acc : 1.0\n",
      "Loss : 0.09387664 Acc : 1.0\n",
      "Loss : 0.08794436 Acc : 1.0\n",
      "Loss : 0.0846411 Acc : 1.0\n",
      "Loss : 0.07990156 Acc : 1.0\n",
      "Loss : 0.07577444 Acc : 1.0\n",
      "Loss : 0.07129092 Acc : 1.0\n",
      "Loss : 0.06882566 Acc : 1.0\n",
      "Loss : 0.06515035 Acc : 1.0\n",
      "Loss : 0.061755672 Acc : 1.0\n",
      "Loss : 0.0591842 Acc : 1.0\n",
      "Loss : 0.05599135 Acc : 1.0\n",
      "Loss : 0.053114764 Acc : 1.0\n",
      "Loss : 0.051293895 Acc : 1.0\n",
      "Loss : 0.04814377 Acc : 1.0\n",
      "Loss : 0.046257555 Acc : 1.0\n",
      "Loss : 0.044186633 Acc : 1.0\n",
      "Loss : 0.042090308 Acc : 1.0\n",
      "Loss : 0.03987445 Acc : 1.0\n",
      "Loss : 0.038203344 Acc : 1.0\n",
      "Loss : 0.036604665 Acc : 1.0\n",
      "Loss : 0.034491196 Acc : 1.0\n",
      "Loss : 0.03299989 Acc : 1.0\n",
      "Loss : 0.029953383 Acc : 1.0\n",
      "Loss : 0.028649906 Acc : 1.0\n",
      "Loss : 0.027536027 Acc : 1.0\n",
      "Loss : 0.025843862 Acc : 1.0\n",
      "Loss : 0.024774045 Acc : 1.0\n",
      "Loss : 0.02382671 Acc : 1.0\n",
      "Loss : 0.022700757 Acc : 1.0\n",
      "Loss : 0.021473251 Acc : 1.0\n",
      "Loss : 0.020704865 Acc : 1.0\n",
      "Loss : 0.019695755 Acc : 1.0\n",
      "Loss : 0.4683643 Acc : 0.921875\n",
      "Loss : 0.020414127 Acc : 1.0\n",
      "Loss : 0.019384015 Acc : 1.0\n",
      "Loss : 0.018492335 Acc : 1.0\n",
      "Loss : 0.017767753 Acc : 1.0\n",
      "Loss : 0.016964328 Acc : 1.0\n",
      "Loss : 0.016041668 Acc : 1.0\n",
      "Loss : 0.015348427 Acc : 1.0\n",
      "Loss : 0.014719382 Acc : 1.0\n",
      "Loss : 0.01393857 Acc : 1.0\n",
      "Loss : 0.0133853005 Acc : 1.0\n",
      "Loss : 0.012820859 Acc : 1.0\n",
      "Loss : 0.01218113 Acc : 1.0\n",
      "Loss : 0.011632899 Acc : 1.0\n",
      "Loss : 0.011144862 Acc : 1.0\n",
      "Loss : 0.010559828 Acc : 1.0\n",
      "Loss : 0.010190113 Acc : 1.0\n",
      "Loss : 0.009751936 Acc : 1.0\n",
      "Loss : 0.009218056 Acc : 1.0\n",
      "Loss : 0.008879015 Acc : 1.0\n",
      "Loss : 0.008471858 Acc : 1.0\n",
      "Loss : 0.008070208 Acc : 1.0\n",
      "Loss : 0.007676484 Acc : 1.0\n",
      "Loss : 0.007432681 Acc : 1.0\n",
      "Loss : 0.0070301937 Acc : 1.0\n",
      "Loss : 0.0067113424 Acc : 1.0\n",
      "Loss : 0.0064974753 Acc : 1.0\n",
      "Loss : 0.0061816378 Acc : 1.0\n",
      "Loss : 0.005912504 Acc : 1.0\n",
      "Loss : 0.005652261 Acc : 1.0\n",
      "Loss : 0.00544078 Acc : 1.0\n",
      "Loss : 0.0051667434 Acc : 1.0\n",
      "Loss : 0.004983657 Acc : 1.0\n",
      "Loss : 0.0047532273 Acc : 1.0\n",
      "Loss : 0.0045069307 Acc : 1.0\n",
      "Loss : 0.004342854 Acc : 1.0\n",
      "Loss : 0.0041485876 Acc : 1.0\n",
      "Loss : 0.003955881 Acc : 1.0\n",
      "Loss : 0.003791899 Acc : 1.0\n",
      "Loss : 0.0036524844 Acc : 1.0\n",
      "Loss : 0.53050053 Acc : 0.921875\n",
      "Loss : 0.004332022 Acc : 1.0\n",
      "Loss : 0.004207989 Acc : 1.0\n",
      "Loss : 0.0040745735 Acc : 1.0\n",
      "Loss : 0.0039117234 Acc : 1.0\n",
      "Loss : 0.0038571963 Acc : 1.0\n",
      "Loss : 0.0036786837 Acc : 1.0\n",
      "Loss : 0.0035397392 Acc : 1.0\n",
      "Loss : 0.0034725908 Acc : 1.0\n",
      "Loss : 0.0033567697 Acc : 1.0\n",
      "Loss : 0.0032170485 Acc : 1.0\n",
      "Loss : 0.0031092032 Acc : 1.0\n",
      "Loss : 0.003002531 Acc : 1.0\n",
      "Loss : 0.0028869777 Acc : 1.0\n",
      "Loss : 0.0028033245 Acc : 1.0\n",
      "Loss : 0.0027129771 Acc : 1.0\n",
      "Loss : 0.002603236 Acc : 1.0\n",
      "Loss : 0.002520902 Acc : 1.0\n",
      "Loss : 0.0024363338 Acc : 1.0\n",
      "Loss : 0.0023522882 Acc : 1.0\n",
      "Loss : 0.002269532 Acc : 1.0\n",
      "Loss : 0.0021962074 Acc : 1.0\n",
      "Loss : 0.0021089143 Acc : 1.0\n",
      "Loss : 0.0020451434 Acc : 1.0\n",
      "Loss : 0.0019771503 Acc : 1.0\n",
      "Loss : 0.0019013408 Acc : 1.0\n",
      "Loss : 0.0018212418 Acc : 1.0\n",
      "Loss : 0.0017715538 Acc : 1.0\n",
      "Loss : 0.0017020213 Acc : 1.0\n",
      "Loss : 0.0016380012 Acc : 1.0\n",
      "Loss : 0.0015848004 Acc : 1.0\n",
      "Loss : 0.0015301609 Acc : 1.0\n",
      "Loss : 0.0014665372 Acc : 1.0\n",
      "Loss : 0.0014170122 Acc : 1.0\n",
      "Loss : 0.0013704811 Acc : 1.0\n",
      "Loss : 0.0013135012 Acc : 1.0\n",
      "Loss : 0.0012691865 Acc : 1.0\n",
      "Loss : 0.001224468 Acc : 1.0\n",
      "Loss : 0.0011736155 Acc : 1.0\n",
      "Loss : 0.0011354028 Acc : 1.0\n",
      "Loss : 0.0010944833 Acc : 1.0\n",
      "Loss : 0.0010545595 Acc : 1.0\n",
      "Loss : 0.001011028 Acc : 1.0\n",
      "Loss : 0.0009717352 Acc : 1.0\n",
      "Loss : 0.0009364327 Acc : 1.0\n",
      "Loss : 0.00089962344 Acc : 1.0\n",
      "Loss : 0.00086795294 Acc : 1.0\n",
      "Loss : 0.0008332906 Acc : 1.0\n",
      "Loss : 0.00080129236 Acc : 1.0\n",
      "Loss : 0.0007422976 Acc : 1.0\n",
      "Loss : 0.0007148809 Acc : 1.0\n",
      "Loss : 0.00068777235 Acc : 1.0\n",
      "Loss : 0.0006596411 Acc : 1.0\n",
      "Loss : 0.0006311611 Acc : 1.0\n",
      "Loss : 0.00061030674 Acc : 1.0\n",
      "Loss : 0.00058707304 Acc : 1.0\n",
      "Loss : 0.00056319276 Acc : 1.0\n",
      "Loss : 0.0005423948 Acc : 1.0\n",
      "Loss : 0.000521131 Acc : 1.0\n",
      "Loss : 0.00049662363 Acc : 1.0\n",
      "Loss : 0.00048062945 Acc : 1.0\n",
      "Loss : 0.00046143835 Acc : 1.0\n",
      "Loss : 0.00044152822 Acc : 1.0\n",
      "Loss : 0.00042409025 Acc : 1.0\n",
      "Loss : 0.00040976127 Acc : 1.0\n",
      "Loss : 0.00038972165 Acc : 1.0\n",
      "Loss : 0.00037581113 Acc : 1.0\n",
      "Loss : 0.00036151503 Acc : 1.0\n",
      "Loss : 0.00034666198 Acc : 1.0\n",
      "Loss : 0.00033117377 Acc : 1.0\n",
      "Loss : 0.00031846346 Acc : 1.0\n",
      "Loss : 0.00030479964 Acc : 1.0\n",
      "Loss : 0.00029296233 Acc : 1.0\n",
      "Loss : 0.00028160718 Acc : 1.0\n",
      "Loss : 0.0002693134 Acc : 1.0\n",
      "Loss : 0.0002580008 Acc : 1.0\n",
      "Loss : 0.00024804563 Acc : 1.0\n",
      "Loss : 0.00023727468 Acc : 1.0\n",
      "Loss : 0.00022709212 Acc : 1.0\n",
      "Loss : 0.00021888154 Acc : 1.0\n",
      "Loss : 0.00020986641 Acc : 1.0\n",
      "Loss : 0.00020033722 Acc : 1.0\n",
      "Loss : 0.00019297376 Acc : 1.0\n",
      "Loss : 0.00018461393 Acc : 1.0\n",
      "Loss : 0.00017676616 Acc : 1.0\n",
      "Loss : 2.610477 Acc : 0.703125\n",
      "Loss : 5.7732744 Acc : 0.3125\n",
      "Loss : 1.0375547 Acc : 0.875\n",
      "Loss : 3.0174599 Acc : 0.625\n",
      "Loss : 5.0385704 Acc : 0.34375\n",
      "Loss : 4.969886 Acc : 0.328125\n",
      "Loss : 5.9538307 Acc : 0.140625\n",
      "Loss : 1.5104511 Acc : 0.78125\n",
      "Loss : 0.0007766918 Acc : 1.0\n",
      "Loss : 0.00076809665 Acc : 1.0\n",
      "Loss : 0.0007954126 Acc : 1.0\n",
      "Loss : 0.00074962195 Acc : 1.0\n",
      "Loss : 0.0007482725 Acc : 1.0\n",
      "Loss : 0.0007575584 Acc : 1.0\n",
      "Loss : 0.0007395752 Acc : 1.0\n",
      "Loss : 0.0007381696 Acc : 1.0\n",
      "Loss : 0.00074119074 Acc : 1.0\n",
      "Loss : 0.0007234211 Acc : 1.0\n",
      "Loss : 0.00072179607 Acc : 1.0\n",
      "Loss : 0.00073260616 Acc : 1.0\n",
      "Loss : 0.00071412936 Acc : 1.0\n",
      "Loss : 0.0007032653 Acc : 1.0\n",
      "Loss : 0.0007067029 Acc : 1.0\n",
      "Loss : 0.0007058965 Acc : 1.0\n",
      "Loss : 0.00069931353 Acc : 1.0\n",
      "Loss : 0.00070572353 Acc : 1.0\n",
      "Loss : 0.00069854263 Acc : 1.0\n",
      "Loss : 0.0006824924 Acc : 1.0\n",
      "Loss : 0.0006896733 Acc : 1.0\n",
      "Loss : 0.0006804113 Acc : 1.0\n",
      "Loss : 0.00065881957 Acc : 1.0\n",
      "Loss : 0.00067559374 Acc : 1.0\n",
      "Loss : 0.00068890245 Acc : 1.0\n",
      "Loss : 0.0006594839 Acc : 1.0\n",
      "Loss : 0.0006682212 Acc : 1.0\n",
      "Loss : 0.00067976513 Acc : 1.0\n",
      "Loss : 0.0006557016 Acc : 1.0\n",
      "Loss : 0.0006579144 Acc : 1.0\n",
      "Loss : 0.0006625685 Acc : 1.0\n",
      "Loss : 0.00065117993 Acc : 1.0\n",
      "Loss : 0.0006330593 Acc : 1.0\n",
      "Loss : 0.00065450615 Acc : 1.0\n",
      "Loss : 0.0006402309 Acc : 1.0\n",
      "Loss : 0.0006397859 Acc : 1.0\n",
      "Loss : 0.0006509231 Acc : 1.0\n",
      "Loss : 0.0006425786 Acc : 1.0\n",
      "Loss : 0.00063277734 Acc : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.00063530565 Acc : 1.0\n",
      "Loss : 0.0006347659 Acc : 1.0\n",
      "Loss : 0.00061780424 Acc : 1.0\n",
      "Loss : 0.00061775395 Acc : 1.0\n",
      "Loss : 0.0006309865 Acc : 1.0\n",
      "Loss : 0.0006126513 Acc : 1.0\n",
      "Loss : 0.0006173331 Acc : 1.0\n",
      "Loss : 0.00062220276 Acc : 1.0\n",
      "Loss : 0.00060111925 Acc : 1.0\n",
      "Loss : 0.0006040791 Acc : 1.0\n",
      "Loss : 0.0006162666 Acc : 1.0\n",
      "Loss : 0.0005956837 Acc : 1.0\n",
      "Loss : 0.00059084385 Acc : 1.0\n",
      "Loss : 0.0006079137 Acc : 1.0\n",
      "Loss : 0.0005881039 Acc : 1.0\n",
      "Loss : 0.0005828412 Acc : 1.0\n",
      "Loss : 0.0005877464 Acc : 1.0\n",
      "Loss : 0.00057752826 Acc : 1.0\n",
      "Loss : 0.000588932 Acc : 1.0\n",
      "Loss : 0.00057342544 Acc : 1.0\n",
      "Loss : 0.00056942285 Acc : 1.0\n",
      "Loss : 0.00057567784 Acc : 1.0\n",
      "Loss : 0.0005675185 Acc : 1.0\n",
      "Loss : 0.0005654964 Acc : 1.0\n",
      "Loss : 0.0005598675 Acc : 1.0\n",
      "Loss : 0.0005659713 Acc : 1.0\n",
      "Loss : 0.00055333495 Acc : 1.0\n",
      "Loss : 0.0005506653 Acc : 1.0\n",
      "Loss : 0.00054999714 Acc : 1.0\n",
      "Loss : 0.00053978973 Acc : 1.0\n",
      "Loss : 0.0005375053 Acc : 1.0\n",
      "Loss : 0.0005355285 Acc : 1.0\n",
      "Loss : 0.0005203319 Acc : 1.0\n",
      "Loss : 0.00052222126 Acc : 1.0\n",
      "Loss : 0.00052156043 Acc : 1.0\n",
      "Loss : 0.0005095639 Acc : 1.0\n",
      "Loss : 0.00051003497 Acc : 1.0\n",
      "Loss : 0.00051170273 Acc : 1.0\n",
      "Loss : 0.0004960648 Acc : 1.0\n",
      "Loss : 0.00049537193 Acc : 1.0\n",
      "Loss : 0.00048884854 Acc : 1.0\n",
      "Loss : 0.00048085226 Acc : 1.0\n",
      "Loss : 0.00047012692 Acc : 1.0\n",
      "Loss : 0.0004703577 Acc : 1.0\n",
      "Loss : 0.0004601383 Acc : 1.0\n",
      "Loss : 0.00044988748 Acc : 1.0\n",
      "Loss : 0.00046063907 Acc : 1.0\n",
      "Loss : 0.00044560898 Acc : 1.0\n",
      "Loss : 0.0004323527 Acc : 1.0\n",
      "Loss : 0.00042273453 Acc : 1.0\n",
      "Loss : 0.00043032147 Acc : 1.0\n",
      "Loss : 0.00040879293 Acc : 1.0\n",
      "Loss : 0.00040335994 Acc : 1.0\n",
      "Loss : 0.00040958583 Acc : 1.0\n",
      "Loss : 0.00039514515 Acc : 1.0\n",
      "Loss : 0.00038409658 Acc : 1.0\n",
      "Loss : 0.00038456387 Acc : 1.0\n",
      "Loss : 0.00036795723 Acc : 1.0\n",
      "Loss : 0.0003611722 Acc : 1.0\n",
      "Loss : 0.0003646653 Acc : 1.0\n",
      "Loss : 0.00035397388 Acc : 1.0\n",
      "Loss : 0.00033839635 Acc : 1.0\n",
      "Loss : 0.00034124323 Acc : 1.0\n",
      "Loss : 0.0003263956 Acc : 1.0\n",
      "Loss : 0.0003163536 Acc : 1.0\n",
      "Loss : 0.00031368717 Acc : 1.0\n",
      "Loss : 0.00029839773 Acc : 1.0\n",
      "Loss : 0.0002914204 Acc : 1.0\n",
      "Loss : 0.00029274807 Acc : 1.0\n",
      "Loss : 0.00028079894 Acc : 1.0\n",
      "Loss : 0.00026954792 Acc : 1.0\n",
      "Loss : 0.00026910476 Acc : 1.0\n",
      "Loss : 0.00025803427 Acc : 1.0\n",
      "Loss : 0.00024899712 Acc : 1.0\n",
      "Loss : 0.0002447588 Acc : 1.0\n",
      "Loss : 0.00023635101 Acc : 1.0\n",
      "Loss : 0.00022758925 Acc : 1.0\n",
      "Loss : 0.00022307712 Acc : 1.0\n",
      "Loss : 0.00021730605 Acc : 1.0\n",
      "Loss : 0.00020800598 Acc : 1.0\n",
      "Loss : 0.00020253836 Acc : 1.0\n",
      "Loss : 0.00019702976 Acc : 1.0\n",
      "Loss : 0.00018678163 Acc : 1.0\n",
      "Loss : 0.00018119099 Acc : 1.0\n",
      "Loss : 0.00017784069 Acc : 1.0\n",
      "Loss : 0.00016911386 Acc : 1.0\n",
      "Loss : 0.000165013 Acc : 1.0\n",
      "Loss : 0.00016192897 Acc : 1.0\n",
      "Loss : 0.00015379608 Acc : 1.0\n",
      "Loss : 0.00014724614 Acc : 1.0\n",
      "Loss : 0.00014416204 Acc : 1.0\n",
      "Loss : 0.00013700305 Acc : 1.0\n",
      "Loss : 0.00013249791 Acc : 1.0\n",
      "Loss : 0.00012754765 Acc : 1.0\n",
      "Loss : 0.00012337026 Acc : 1.0\n",
      "Loss : 0.00011786682 Acc : 1.0\n",
      "Loss : 0.00011408981 Acc : 1.0\n",
      "Loss : 0.00011020477 Acc : 1.0\n",
      "Loss : 0.00010505511 Acc : 1.0\n",
      "Loss : 0.0001016431 Acc : 1.0\n",
      "Loss : 9.827019e-05 Acc : 1.0\n",
      "Loss : 9.30292e-05 Acc : 1.0\n",
      "Loss : 9.044222e-05 Acc : 1.0\n",
      "Loss : 8.759636e-05 Acc : 1.0\n",
      "Loss : 8.25602e-05 Acc : 1.0\n",
      "Loss : 7.996574e-05 Acc : 1.0\n",
      "Loss : 7.709005e-05 Acc : 1.0\n",
      "Loss : 7.359599e-05 Acc : 1.0\n",
      "Loss : 7.0141024e-05 Acc : 1.0\n",
      "Loss : 6.882609e-05 Acc : 1.0\n",
      "Loss : 6.5127126e-05 Acc : 1.0\n",
      "Loss : 6.254567e-05 Acc : 1.0\n",
      "Loss : 6.083214e-05 Acc : 1.0\n",
      "Loss : 5.7781304e-05 Acc : 1.0\n",
      "Loss : 5.4881333e-05 Acc : 1.0\n",
      "Loss : 5.1234467e-05 Acc : 1.0\n",
      "Loss : 4.812773e-05 Acc : 1.0\n",
      "Loss : 4.6848152e-05 Acc : 1.0\n",
      "Loss : 4.4672684e-05 Acc : 1.0\n",
      "Loss : 4.2800806e-05 Acc : 1.0\n",
      "Loss : 4.1355455e-05 Acc : 1.0\n",
      "Loss : 3.985795e-05 Acc : 1.0\n",
      "Loss : 3.817046e-05 Acc : 1.0\n",
      "Loss : 3.6788428e-05 Acc : 1.0\n",
      "Loss : 3.5438057e-05 Acc : 1.0\n",
      "Loss : 3.3568027e-05 Acc : 1.0\n",
      "Loss : 3.2329408e-05 Acc : 1.0\n",
      "Loss : 3.1068437e-05 Acc : 1.0\n",
      "Loss : 2.9168597e-05 Acc : 1.0\n",
      "Loss : 2.8127408e-05 Acc : 1.0\n",
      "Loss : 2.7112295e-05 Acc : 1.0\n",
      "Loss : 2.5672507e-05 Acc : 1.0\n",
      "Loss : 2.4672292e-05 Acc : 1.0\n",
      "Loss : 2.3849025e-05 Acc : 1.0\n",
      "Loss : 2.2982913e-05 Acc : 1.0\n",
      "Loss : 2.193613e-05 Acc : 1.0\n",
      "Loss : 2.1196676e-05 Acc : 1.0\n",
      "Loss : 2.0339878e-05 Acc : 1.0\n",
      "Loss : 1.9319168e-05 Acc : 1.0\n",
      "Loss : 1.8901945e-05 Acc : 1.0\n",
      "Loss : 1.8147588e-05 Acc : 1.0\n",
      "Loss : 1.7190203e-05 Acc : 1.0\n",
      "Loss : 1.6787879e-05 Acc : 1.0\n",
      "Loss : 1.6081949e-05 Acc : 1.0\n",
      "Loss : 1.5357393e-05 Acc : 1.0\n",
      "Loss : 1.482282e-05 Acc : 1.0\n",
      "Loss : 1.42957e-05 Acc : 1.0\n",
      "Loss : 1.3552513e-05 Acc : 1.0\n",
      "Loss : 1.2924811e-05 Acc : 1.0\n",
      "Loss : 1.2211427e-05 Acc : 1.0\n",
      "Loss : 1.1566959e-05 Acc : 1.0\n",
      "Loss : 1.1008173e-05 Acc : 1.0\n",
      "Loss : 1.0717602e-05 Acc : 1.0\n",
      "Loss : 1.0184892e-05 Acc : 1.0\n",
      "Loss : 9.780702e-06 Acc : 1.0\n",
      "Loss : 9.490132e-06 Acc : 1.0\n",
      "Loss : 9.141821e-06 Acc : 1.0\n",
      "Loss : 8.726454e-06 Acc : 1.0\n",
      "Loss : 8.521565e-06 Acc : 1.0\n",
      "Loss : 8.214232e-06 Acc : 1.0\n",
      "Loss : 7.8975845e-06 Acc : 1.0\n",
      "Loss : 7.640541e-06 Acc : 1.0\n",
      "Loss : 7.3350698e-06 Acc : 1.0\n",
      "Loss : 7.063126e-06 Acc : 1.0\n",
      "Loss : 6.8638237e-06 Acc : 1.0\n",
      "Loss : 6.610506e-06 Acc : 1.0\n",
      "Loss : 6.3814023e-06 Acc : 1.0\n",
      "Loss : 6.224941e-06 Acc : 1.0\n",
      "Loss : 5.988386e-06 Acc : 1.0\n",
      "Loss : 5.8132982e-06 Acc : 1.0\n",
      "Loss : 5.656837e-06 Acc : 1.0\n",
      "Loss : 5.4556726e-06 Acc : 1.0\n",
      "Loss : 5.273135e-06 Acc : 1.0\n",
      "Loss : 5.1166735e-06 Acc : 1.0\n",
      "Loss : 5.003052e-06 Acc : 1.0\n",
      "Loss : 4.844728e-06 Acc : 1.0\n",
      "Loss : 4.703168e-06 Acc : 1.0\n",
      "Loss : 4.6212117e-06 Acc : 1.0\n",
      "Loss : 4.455437e-06 Acc : 1.0\n",
      "Loss : 4.3325035e-06 Acc : 1.0\n",
      "Loss : 4.2579977e-06 Acc : 1.0\n",
      "Loss : 4.125751e-06 Acc : 1.0\n",
      "Loss : 4.01958e-06 Acc : 1.0\n",
      "Loss : 3.952525e-06 Acc : 1.0\n",
      "Loss : 3.8426297e-06 Acc : 1.0\n",
      "Loss : 3.7606733e-06 Acc : 1.0\n",
      "Loss : 3.6973438e-06 Acc : 1.0\n",
      "Loss : 3.5874482e-06 Acc : 1.0\n",
      "Loss : 3.5203932e-06 Acc : 1.0\n",
      "Loss : 3.4607888e-06 Acc : 1.0\n",
      "Loss : 3.3862832e-06 Acc : 1.0\n",
      "Loss : 3.2968765e-06 Acc : 1.0\n",
      "Loss : 3.252173e-06 Acc : 1.0\n",
      "Loss : 3.181393e-06 Acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.nn.rnn_cell import BasicRNNCell\n",
    "tf.reset_default_graph()\n",
    "sess=tf.Session()\n",
    "\n",
    "WINDOW_SIZE = 100\n",
    "signal = tf.placeholder(tf.float32, [None, WINDOW_SIZE, 1])\n",
    "channel = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "rnn = BasicRNNCell(num_units=10)\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(rnn, signal, dtype=tf.float32)\n",
    "# Get output of RNN sequence\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "\n",
    "print(last)\n",
    "print(state)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([10, 10], stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "logits_out = tf.matmul(last, weight) + bias\n",
    "\n",
    "# Loss function\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=channel)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(channel, tf.int64)), tf.float32))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(0.005)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "# Start training\n",
    "data_generator = generator_data(train, batch_size=64, WINDOW_SIZE=WINDOW_SIZE)\n",
    "for epoch in range(int(5000)):\n",
    "    (x_train, y_train) = next(data_generator)\n",
    "    # Shuffle training data\n",
    "    shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n",
    "    x_train = x_train[shuffled_ix]\n",
    "    y_train = y_train[shuffled_ix]\n",
    "    sess.run(train_step, feed_dict={signal:x_train,channel:y_train})\n",
    "    \n",
    "    if epoch % 100:\n",
    "        # Run loss and accuracy for training\n",
    "        temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict={signal:x_train,channel:y_train})\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_accuracy.append(temp_train_acc)\n",
    "        print(\"Loss :\", temp_train_loss, \"Acc :\", temp_train_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "WINDOW_SIZE = 10\n",
    "def generator_data_test(test, batch_size, WINDOW_SIZE = 100):\n",
    "    X_train = []\n",
    "    for i in range(WINDOW_SIZE,train.shape[0]):\n",
    "        #print(\"DEBUG\", i)\n",
    "        feature = [[i]for i in train.to_numpy()[i-WINDOW_SIZE:i, 1]]\n",
    "        X_train.append(feature)\n",
    "        if (i-WINDOW_SIZE+1) % batch_size == 0:\n",
    "           # print(len(X_train))\n",
    "            #print(len(Y_train))\n",
    "            data_generate = (np.array(X_train))\n",
    "            X_train = [] \n",
    "            yield data_generate\n",
    "test_data_generator = generator_data_test(test, batch_size, WINDOW_SIZE = 100)\n",
    "x_test = next(test_data_generator)  # x_test.shape(batchsize, 100)  \n",
    "x_test_predict = sess.run(tf.nn.softmax(logits_out), feed_dict={signal:x_test})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
